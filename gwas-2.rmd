---
title: 'GWAS tutorial: Imputation and population structure'
author: Patrick Breheny, Anna Reisetter, and Tabitha Peter
date: '`r format(Sys.Date(), "%B %d, %Y")`'
extra_dependencies: ["centernot"]
editor_options: 
  chunk_output_type: console
---

```{r knitr_setup, include=FALSE, purl=FALSE}
library(knitr)
library(kableExtra)
set.seed(1)
knitr::opts_knit$set(aliases=c(h = 'fig.height', w = 'fig.width'))
knitr::opts_chunk$set(comment="#", message=FALSE, collapse=TRUE, cache=FALSE, tidy=FALSE, fig.align="center")
knitr::knit_hooks$set(small.mar = function(before, options, envir) {
  if (before) par(mar = c(4, 4, .1, .1))
})
```

# Before you begin...

## Motivating questions 

1. What assumptions does a GWAS make about a population? 

2. What happens when these assumptions are not met? 

3. How can we make adjustments to our models to accommodate complex data that do not meet our classical assumptions? 


## Objectives of this module 

When you have completed this module, you will know: 

1. ... how to summarize the assumptions (biological and statistical) that a GWAS makes about the individuals in a study and the structure of the data. 

2. ... how to identify instances when your data show evidence of **not** meeting these assumptions. 

3. ... what tools are available for analyzing complex data that do not meet the basic GWAS assumptions. 

4. ... how to implement the tools mentioned above using `R`

## Set up 

To begin, read in the **qc** data from earlier step (refer back to the "Data" page of the tutorial). 

```{r read}
# Load our packages  (same ones mentioned in the data module)
library(snpStats)
library(SNPRelate)
library(data.table)
library(tidyverse)

## Register cores for parallel processing - very helpful if you're on a laptop
library(doSNOW)
registerDoSNOW(makeCluster(4))

# Read in data object created in previous module 
obj <- readRDS('data/gwas-qc.rds')
obj$genotypes
# double check dimensions 
dim(obj$map)
cs <- col.summary(obj$genotypes) # hold onto this --- will need it later 
```

# Imputation

In a genetics research context, most observations (*e.g* patients or subjects) will have missing values for at least one SNP. A common method of dealing with missing SNP data is imputation. 

## Why impute? 

There are two main reasons that one would use imputation:

1. To replace missing SNP values with what these values are predicted to be, based upon a person's (or subject's) available SNP values near the loci of interest. For instance, suppose that in a given data set, patient A is missing SNP 123. This value for patient A could be imputed based on the patient's other SNP values at loci near 123. 

2. To infer values for SNPs that were not measured at all for any patients (subjects). This would be the case if one was to merge data from studies that examined different loci. For instance, suppose I am merging data from studies A and B. Suppose further that study A measured SNP 123 for all patients, but study B did not. In the merged data, I may need to impute values for SNP 123 for all patients in the study B data. 

For the purposes of this tutorial, let us limit ourselves to scenario (1). 

Recall that in the QC step of our analysis, we excluded SNPs with $\ge 90\%$ missingness. However, there may still be SNPs with some missingness. SNPs that are not missing are described as "called." The **call rate** is the proportion of genotypes that are called (see `snpStats::col.summary()` documentation for details). Therefore, a call rate of 1 indicates that a SNP has no missing values.

By examining the call rate information, we will first check how many SNPs in our qc'd data set have some missing data:

```{r, any-missing}
table(cs$Call.rate == 1)
```

 This tells us that `r table(cs$Call.rate == 1)['TRUE']` SNPs have no missingness, but `r table(cs$Call.rate == 1)['FALSE']` still have some missingness (albeit less than 10\%.) We will try to impute values for these SNPs using the `snp.imputation()` function from `snpStats`. `snp.imputation()` has numerous options that can be tweaked according to the needs of a specific problem. We will perform a basic imputation for now; see the R documentation for more details.
 
 The package `snpStats` uses a two step imputation procedure. First, the function determines a set of "tag" SNPS. These tag SNPs are used to predict the missing SNP values and to generate prediction rules for the missing SNPs. Second, these prediction rules are applied to the supplied genotype matrix where missing SNP values are imputed. 
 
**N.B** In the case where there is insufficient data or a lack of tagging SNPs, it is possible for the generated prediction rules to fail at yielding predictions. We will see this occur as we go through our example. 

## Implementation/tools 

To implement imputation, we will use a three step approach:

  1. Determine tag SNPs
  
  2. Use tag SNPs to generate prediction rules
  
  3. Apply these prediction rules to our genotype matrix and "fill in the blanks"
  
### Determine the 'tag' SNPs

A SNP is called a 'tag' SNP if it is being used to represent (or mark) a specific haplotype. Typically, a tag SNP is in a region of the genome with high linkage disequilibrium. As you will recall, the areas of the genome where there appears to be non-random association of alleles in the population are the areas of our interest. 

We want to find these tag SNPs and use them to help us impute missing values. There are many algorithms that can be used to identify tag SNPs - a deep dive into this will take you into computational complexity theory. Check out [the Wikipedia page](https://en.wikipedia.org/wiki/Tag_SNP#Steps_for_tag_SNP_selection) if you want to take that plunge -- for our purposes here, we will use the same function in the `snpStats` package to both identify tag SNPs and generate prediction rules. 

### Use tag SNPs to generate prediction rules 

As mentioned above, I use the `snp.imputation()` function to both identify the tag SNPs and generate the prediction rules for imputing the missing values:  

```{r, gen_pred_rules}

?snp.imputation # check out the help file -- there is a lot here 


# determine tagging SNPs. Note: this can take a few minutes
rules <- snpStats::snp.imputation(obj$genotypes, minA=0)
# NB: minA is a threshold of the amount of existing data needed to impute missing values. Higher minA is a more stringent threshold. Here, we are setting the most loose threshold possible - the default threshold value is 5.
```

### Fill in the blanks

Now that we have tagged important SNPs and created rules for imputation, we can actually implement the imputation with the `impute.snps()` function. This will "fill in the blanks" in our data set, decreasing the number of missing values. 

```{r fill_in}
rules_imputed <- impute.snps(rules, obj$genotypes, as.numeric = FALSE) 
# returns SnpMatrix object


# how many SNPs still have missing data after imputation?
cs_rules_imputed <- col.summary(rules_imputed)
table(cs_rules_imputed$Call.rate == 1) # TRUE = SNPs with no missing values


```


```{r fill_in_numeric, eval=FALSE, include=FALSE}
rules_imputed_numeric <- impute.snps(rules, obj$genotypes, as.numeric = TRUE)
# returns numeric matrix (see help documentation)


# compare this column summary to the numeric format 
# NB: using `apply()` exhausts memory, but `sapply()` will work: 
call_rates <- sapply(X = 1:ncol(rules_imputed_numeric),
                    FUN = function(x){sum(!is.na(rules_imputed_numeric[,x]))/nrow(rules_imputed_numeric)})

```


Notice that even after going through the imputation process, there are still `r table(cs_rules_imputed$Call.rate == 1)[FALSE]` missing values in this data set. This is not unusual for genetics data. It is not uncommon for there to be SNPs that are both missing a notable amount of values and located "far" from surrounding SNPs. In such situations, it is not possible to impute values -- we do not know enough to impute a value in these cases. However, we also know that we cannot have any missing values in a regression model (which is where we are headed in our analysis). So, for the missing values that remain after imputation, we can use this case-by-case approach: 

1.  Does the SNP have $ > 50 \%$ missingness? If so, exclude it from the analysis. We do not know enough to impute a value, and there is not enough information in this SNP for us to learn anything about our outcome(s) of interest.

 2. For SNPs with $ \leq 50 \%$ missingness, I can do a simple mean imputation - that is, take the mean value of that SNP across all the genotypes, and use this mean value to "fill in the blanks." I give an example of this in just a bit. 
 
 Let's see how many SNPs in our example data have $ \leq 50 \%$ missingness. 
 

```{r missings}
# how many of the SNPs with some missingness are missing for <= 50% of observations? 
# NB: less than 50% missingness -> call rate at or above 50% 
table(cs_rules_imputed$Call.rate >= 0.5)

# ... and with the numeric form: 
# sum(call_rates >= 0.5) 
```

 So we notice that all of our SNPs with remaining missing data are missing values for no more than half of the patients in the study. I will use the simple mean imputation to address this missingness - no more SNPs need to be eliminated from the analysis. 

A couple of notes about this approach: 

  - The cutoff of $50 \%$ is an arbitrary choice on my part. You could choose $60 \%$ or $75 \%$ of you wanted to... it may even be best to examine what happens to the results for your specific data set across several cutoff values.
  
  - Of course, the simple mean imputation only applies when you are talking about a continuous trait. For a categorical outcome, you would need another approach. 


### Mean and Mode imputation 

We can look at the $R^2$ values to check the imputation quality. This vignette has additional information about accessing the $R^2$ values and evaluating imputation quality with `snpstats`:

* [Imputation Vignette](https://www.bioconductor.org/packages/release/bioc/vignettes/snpStats/inst/doc/imputation-vignette.pdf)


Again, many statistical methods we may want to apply to these data which cannot handle any missingness. As mentioned above, one simplistic yet reasonable thing to do for these values is to replace them with their HWE expected value (i.e. the average of that SNP). 

There are a number of ways to code this, but with `snpStats` it will require us to convert data from the `snpStats` `SnpMatrix` format to numeric and back. Generally, the entire `SnpMatrix` object will be too large for `R` to read into memory, so I will do this one SNP at a time in a loop. 

We can impute the mean or mode (i.e. 0, 1, or 2) of these SNPs. The mode will allow us to convert the imputed data back to PLINK file formats, but if that's our objective, we'll also have to check all of our `snpStats` imputed values and round them where necessary, which can take a while. If we don't need to interact with PLINK or PLINK-related R packages, we can leave non-integer genotype values as they are, and simply impute the remaining values using the mean. This will allow us to only work on the SNPs that have remaining missingness and will be faster, so that's what I'm going to do. 

```{r, impute_mean}
(missing <- table(cs_rules_imputed$Call.rate == 1)) 
# use this to verify things are being subset appropriately, dimensions match, etc.

# identify which SNPs have missingness
to_impute <- which(cs_rules_imputed$Call.rate < 1)
# one more check 
length(to_impute)

# subset to a SnpMatrix object with only SNPs with some missingness - we want to loop
# over these to replace missing values with the mean, but it's a waste of time
# to loop over the SNPs with no missingness so I'm subsetting to be efficient 
miss <- rules_imputed[, to_impute]
dim(miss)

# this is done in a way where only one SNP at a time is converted to a numeric - otherwise this
# is computationally too expensive in R
imputed_mean <- sapply(1:ncol(miss), function(x){
  s <- drop(as(miss[,x], 'numeric'))
  idx <- which(is.na(s)) # identify missing values in a numeric vector
  s[idx] <- mean(s, na.rm = TRUE) # replace missing values with that SNP mean
  raw <- snpStats::mean2g(s) # convert the numeric SNP vector back to its raw, memory friendly form
  return(raw)
})
# check 
dim(imputed_mean) 

fully_imputed <- rules_imputed 
# NB:  I'm making a copy of the imputed data with residual missingness so I can compare the snpstats "rules_imputed" data with the "fully_imputed" data

fully_imputed@.Data[, to_impute] <- imputed_mean # replace the portion of the imputed matrix with missingness with the HWE-imputed version from our loop
```


```{r working_code1, eval=FALSE, include=FALSE}
#' A function for simple mean imputation of continuous SNP data
#' @param x A column of data from a SNP matrix
#' @return raw A mean-imputed version of that data in raw form 
imputed_mean <- function(x){
  # get data from raw form into numeric - need this for calculating the mean 
  s <- drop(as(x, 'numeric'))
  # identify missing values in a numeric vector
  miss_idx <- which(is.na(s)) 
  # replace missing values with that SNP mean
  s[miss_idx] <- mean(s, na.rm = TRUE) 
  # convert the numeric SNP vector back to its raw, memory friendly form
  raw <- snpStats::mean2g(s) 
  # return raw value 
  return(raw)
}

# FIXME: This exhausts memory 
mean_imputed_dat <- sapply(X = rules_imputed@.Data[,to_impute],
                        FUN = imputed_mean)

```

```{r working_code2, eval=FALSE, include=FALSE}

# identify which SNPs have missingness
to_impute <- which(call_rates < 1)

# Now, I will try to perform the mean imputation on the numeric matrix 

#' A function for simple mean imputation of continuous SNP data
#' @param x A column of data from a SNP matrix
#' @return raw A mean-imputed version of that data in raw form 
imputed_mean <- function(x){
  # identify missing values in a numeric vector
  miss_idx <- which(is.na(x)) 
  # replace missing values with that SNP mean
  x[miss_idx] <- mean(x, na.rm = TRUE) 
  
  return(x)
}

# Test this function 
x <- c(1:3, NA_real_)
imputed_mean(x) # confirm that this is what I expect 


fully_imputed <- sapply(X = rules_imputed_numeric[,to_impute],
                        FUN = imputed_mean)

```

```{r working_code3, eval=FALSE, include=FALSE}
(missing <- table(cs_rules_imputed$Call.rate == 1)) 
# use this to verify things are being subset appropriately, dimensions match, etc.

# identify which SNPs have missingness
to_impute <- which(cs_rules_imputed$Call.rate < 1)
# one more check 
length(to_impute)

# NB:  Make a copy of the imputed data with residual missingness 
#   Later, I will want to compare the "rules_imputed" data with the "fully_imputed" data
fully_imputed <- rules_imputed 


# now, I implement the simple mean imputation to obtain a fully imputed data set 
# NB: this is done in a way where only one SNP at a time is converted to a numeric
#   otherwise, this is computationally too expensive in R
fully_imputed@.Data[, to_impute] <- sapply(1:length(to_impute), function(x){
  # notice the double-indexing below: x is iterating over only the columns that contain missing values
  s <- drop(as(fully_imputed@.Data[, to_impute][,x], 'numeric'))
  idx <- which(is.na(s)) # identify missing values in a numeric vector
  s[idx] <- mean(s, na.rm = TRUE) # replace missing values with that SNP mean
  raw <- snpStats::mean2g(s) # convert the numeric SNP vector back to its raw, memory friendly form
  return(raw)
})
# check 
dim(imputed_mean) 

fully_imputed <- rules_imputed 
# NB:  I'm making a copy of the imputed data with residual missingness so I can compare the snpstats "rules_imputed" data with the "fully_imputed" data

fully_imputed@.Data[, to_impute] <- imputed_mean # replace the portion of the imputed matrix with missingness with the HWE-imputed version from our loop
```


Below is code to impute using the mode, and to check and convert any `snpStats` imputed values to the nearest integer values where necessary. This takes about 45 minutes (which is annoying), but may be worthwhile if your data isn't very big, or if using the PLINK formats is of interest. The overall structure is similar to the mean-imputation code above, but with some key differences. We will also have to re-check for monomorphic SNPs and potentially re-subset our data if we use this type of imputation. We also need to loop over all of the SNPs to check for non-integer values - I can't simply loop over the chunk with missingness as above.

```{r, impute_mode, eval = FALSE}
# R doesn't have a built in `mode` function so we'll have to write our own
mode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# this takes ~ 45 min
start <- Sys.time()
imputed_mode <- sapply(1:ncol(rules_imputed), function(x){
  s <- drop(as(imputed[,x], 'numeric'))
  idx <- which(is.na(s)) # look for missing values in a numeric vector
  if (length(idx) > 0) s[idx] <- mode(s) # replace missing values with that SNP mode
  s1 <- snpStats::mean2g(s) %>%
    snpStats::g2post() # s1 contains the posterior probabilities of the possible genotypes
  to_round <- apply(s1, 1, function(z) length((z[z > 0] > 1))) 
  idx2 <- which(to_round > 1)
  s1[idx2,] <- t(apply(t(s1[idx2,, drop = FALSE]), 2, function(y) list(c(1, 0, 0),
                                                                       c(0, 1, 0),
                                                                       c(0, 0, 1))[[which.max(y)]])) # impute using the genotype with the highest posterior probability
  raw <- snpStats::mean2g(s1) # convert the numeric SNP vector back to its raw, memory friendly form
  return(raw)
})
stop <- Sys.time()
stop - start
dim(imputed_mode)

imputed3 <- as(imputed_mode, 'raw') # convert to raw

# we need to do another check for monomorphic snps with this type of imputation
cs_rules_imputed3 <- col.summary(imputed3)
table(cs_rules_imputed3$MAF == 0) # some of these have been imputed such that they are now monomorphic! re-subset...
```

Now back to our mean-imputed object, `imputed2`. We have a genotype object with no missingness. Lets double check that we in fact have no missing values and that our labels match, in case something weird happened that didn't throw an error. When working with large genetic data sets it is easy for something like that to occur and go unnoticed, so double checking our preprocessing is almost always worthwhile.

Again, because of the size of our `SnpMatrix` object, this isn't as simple as `sum(is.na(imputed2))`, and the following would throw an error.

```{r example_err, eval=FALSE}
# THIS WILL THROW AN ERROR 
sum(is.na(fully_imputed))
```

But, we can do this using the `snpStats` `col.summary()` function

```{r}
cs_fully_imputed <- snpStats::col.summary(fully_imputed)
table(cs_fully_imputed$Call.rate)
# notice that now, all SNPs are called - yay! 
```


Or, we can check missingness using base `R` functions in multiple chunks `R` can handle. The `col.summary()` method is easier and faster, but this method is more broadly applicable.

```{r, eval=FALSE}
# make sure there are no missing values remaining / count missing values
missing <- 0
chunks <- ceiling(nrow(fully_imputed) / 100) # I'm breaking this up using 100 based on on trial and error but this can be tweaked.
start <- 1
for (i in 1:chunks){
  stop <- min(i*100, nrow(fully_imputed))
  missing <- missing + sum(is.na(fully_imputed[start:stop,]))
  start <- stop + 1
}
missing # should be 0!



# Check for Inf values 
inf <- 0
start <- 1
for (i in 1:chunks) {
  stop <- min(i*100, nrow(fully_imputed))
  inf <- inf + sum(!(is.finite(fully_imputed[start:stop,])))
  start <- stop + 1
}

inf # should be 0 
# FIXME: I have 1054497675 infinite values! SOS! 
```

Now we'll replace our `obj$genotypes` with the fully-imputed version, and make sure the `map` and `fam` objects are appropriately subset and ordered.

```{r, eval=FALSE}
obj$genotypes <- fully_imputed
obj$map <- obj$map[colnames(obj$genotypes),]
stopifnot(all.equal(colnames(obj$genotypes), as.character(obj$map$snp.name)))
obj$fam <- obj$fam[rownames(obj$genotypes),]
stopifnot(all.equal(rownames(obj$genotypes), as.character(obj$fam$pedigree)))
```

Let's save this fully imputed data set for future use in downstream analyses:
```{r save_imp, eval=FALSE}
saveRDS(obj, 'data/gwas-imp.rds')
```

# Imputation (advanced)

A more complex method of imputation involves the use of reference genome panels in addition to the observed data itself. The basic idea is to use known haplotypes, or groups of alleles inherited together, from reference genomes to give us better estimates of unobserved genotypes. These reference panels typically come from the either the 1000 Genomes project or the HapMap project, both of which are maintained by large-scale international organizations that aim to develop haplotype maps of the human genome in diverse populations. 

* [IGSR: The International Genome Sample Resource](https://www.internationalgenome.org/home)
* [International HapMap Project](https://www.genome.gov/10001688/international-hapmap-project)

In addition to allowing us to estimate untyped SNPs as we did above, where our SNPs of interest were typed in our population of interest but we had call rates of less than 1, this method of imputation can also allow us to estimate SNPs that were not genotyped on a particular population at all. This can be useful for combining multiple genetic data sets where different SNPs were typed, or for evaluating associations in distinct genetic populations. 

It's important to be aware that this type of imputation is possible, and commonly done. However, since it involves its own large array of software and expertise, it is probably something you would want to consult with an expert on. The Michigan Imputation Server is a service that will do more complex imputation for you. It also contains information about the various reference panels, their versions, etc. 

* [Michigan Imputation Server](https://imputationserver.sph.umich.edu/index.html#!)


# Population structure

## Concept

In a GWAS context, *population structure* is a kind of relatedness among the individuals represented in the data set. When considering relatedness, it is helpful to define the phrase **identical by descent**. Two alleles from the same locus are identical by descent if they can be traced back from the same allele in an earlier generation. For more information on this concept, check out [this example](https://pbgworks.org/sites/pbgworks.org/files/KinshipMatrixFinal.pdf). 


Population structure is defined by the existence of allele frequency differences that characterize sub-populations and is driven by the combined effects of evolutionary processes such as genetic drift, migration, demographic history, and natural selection.

Population structure is a common phenomenon in genetic data. Varying levels of relatedness are almost always present among genetic samples, even in samples of unrelated individuals and seemingly homogeneous populations. For example, European American, Han Chinese, and recently, cohorts within the UK Biobank data have been shown to exhibit patterns of population and geographic structure despite their seemingly similar subjects.

Population structure is broadly categorized based on whether it describes recent or ancient relatedness. Ancient relatedness describes the presence of a common ancestor many generations previously. The presence of distinct ancestry groups with different allele frequencies in a sample is known as *population stratification* (a.k.a *ancestry differences*). Recent relatedness describes the sharing of a common ancestor only several generations previously. Pedigree-based methods may be used to explicitly model recent relatedness if familial relationships are known. In the absence of known familial relationships, recent relatedness is referred to as *cryptic relatedness*. ^[NB: definitions of the terms `recent` and `ancient` are somewhat subjective and hand-wavy since, in theory, if you look back far enough, everyone shares a common ancestor. However, the idea is that humans migrated, separated, and mated such that over time distinct groups developed allele frequencies different enough to confound an analysis.]



## Population stratification 

Population stratification in particular has been of great concern in genetic studies due to its potential to lead to spurious associations when population structure is associated with differences in both allele frequency and the trait or disease.

As an example, consider a GWAS to assess genetic variants associated with lung cancer in a sample comprised of subjects from two distinct subpopulations, A and B. Assume the minor allele of SNP X is present with higher frequency in subpopulation A compared to subpopulation B, but has no direct effect on lung cancer. Also suppose these subpopulations are geographically segregated in a such a way that subpopulation A is exposed to good air quality, and subpopulation B to poor air quality, and that air quality has a direct effect on lung cancer. A GWAS of data from these subpopulations would find SNP X to be significantly associated with lung cancer, even though we know it has no effect on lung cancer. If subpopulations A and B were not subject to different air qualities, all else being equal, SNP X would not be found to be associated with the phenotype.

Another apocryphal but illustrative example of how population stratification can lead to confounding in genetic studies is linked below.

* [Beware the chopsticks gene](https://www.nature.com/articles/4000662)


## Cryptic relatedness 

An example of cryptic relatedness would be an instance where, unbeknownst to the researcher(s), some of the individuals in a study are first cousins. 

## Tools/implementation 

GWAS (and many statistical tests) assume samples are independent. Cryptic relatedness and population structure can invalidate these tests since the presence of non-independent samples, and thus non-independent errors, can lead to inflated test statistics. 

With this in mind, it is critical to evaluate and account for potential population structure in our data in order to avoid false positives and negatives. 
 
 


```{r, include=FALSE}
imp_obj <- readRDS('data/gwas-imp.rds')
```
 

### PCA example 

One of the simplest and most common methods used to assess and correct for population structure in our data is with PCA. Here, I will present a 'crash course' on the concepts of PCA and give an example with our data.

  Conceptually, PCA can be thought of as extracting the axes of greatest variability in our data. PCA creates linear combinations of a given set of variables; this lets the user represent a lot of correlated variables with a smaller number of uncorrelated variables - so PCA gives us **dimension reduction**, which is really helpful for high dimensional settings like GWAS data.
  
  Connections to different conceptual frameworks:

  - From a linear modeling framework, PCA is an orthogonal linear transformation of a data set.

  - From a machine learning framework, PCA is a type of unsupervised learning. 
  
  A typical PCA includes the following steps: 

  1. Standardize (center and scale) the data.  
  
  2. Calculate the covariance matrix $\textbf{S} = \tilde{X}^T \tilde{X}$, where $\tilde{X}$ is our centered and scaled design matrix from step (1). 
  
  3. Find the eigenvectors and eigenvalues of $\textbf{S}$
  
  4. Calculate the inner products of the columns of $\tilde{X}$ and the associated eigenvectors - these inner products are the **principal components**, whereas the eigenvector coefficients are the **principal component loadings**
  
  5. Determine a number of principal components to keep, and create a feature vector with only these components
  
  6. Plot the feature vector along the principal component axes (as opposed to the axes of the original data space) - these plots can reveal clusters/population structures within the data 

  
Our data will require special tools for PCA because of its size. We'll get there, but to start, I'm going to illustrate some of these concepts using some smaller data with known structure. First, we'll read in the small data and filter out the monomorphic SNPs. Note that this data set contains a known race variable.

```{r, process_admixture}
Data <- read.delim("https://s3.amazonaws.com/pbreheny-data-sets/admixture.txt")
Data[1:5, 1:5]
Race <- Data$Race
XX <- as.matrix(Data[,-1]) # create an all SNP mat
polymorphic <- apply(XX, 2, sd) != 0
X <- XX[,polymorphic] # filter out monomorphic SNPs
dim(X)
# [1] 197  98
table(Race)
```

Next, I'm going to compute the principal components (pcs) and plot a scree plot. This tells us the proportion of variance explained by each of the pcs. PCA is such that the pcs are ordered from those that explain the greatest to least amount of variability. Scree plots are sometimes used to decide how many pcs are appropriate to include. We look for the 'elbow' in the plot, which indicates when the proportion of variance explained by including additional pcs may not be worth the extra df. There are also a number of more complex tests and tools to determine this as well, or often the top 10 pcs are simply used in practice. 
```{r, plot_admixture}
pca <- prcomp(X, center = TRUE, scale = TRUE)
pca$x[1:5, 1:5]
# I'm only plotting the top 10 pcs
plot(1:10, pca$sdev[1:10]^2, type = 'b', ylab = 'Proportion of variance explained', xlab = 'PC', main = 'Scree Plot')

```

We see this elbow point at 3 pcs, which makes sense, given that there are 4 distinct races in this data set (we lose one df when we center our data).

Now we'll plot the first two pcs against each other, and color the data points by the known race of each subject. We expect to see clustering that corresponds to population structure. 

```{r}
gdat <- data.frame(Race = Race, PC1 = pca$x[,1], PC2 = pca$x[, 2])
library(ggplot2)
g <- ggplot(gdat, aes(x = PC1, y = PC2, col = Race)) +
  geom_point() +
  coord_fixed()
plot(g)
```

Indeed, we see that the first two pcs differentiate the racial groups, which cluster together. We can see that PC1 seems to differentiate the European/Japanese populations from the African/African American ones, while PC2 seems to primarily differentiate the European and Japanese populations. Even if we did not have the known race vector to color the points, we could still pick out some clustering from this plot, which indicates underlying structure. If there were no underlying structure in our data, we would expect to see no clustering or systematic pattern in this plot. 

As a counter example to this plot, we can plot the pcs from a random matrix:

```{r}
# I'm going to use the same dimensions as those in the admixture data.
n <- 197
p <- 98
XX <- matrix(rnorm(n * p), n, p)
pca2 <- prcomp(XX, center = TRUE, scale = TRUE)
gdat2 <- data.frame(PC1 = pca2$x[,1], PC2 = pca2$x[, 2])
g <- ggplot(gdat2, aes(x = PC1, y = PC2)) +
  geom_point() +
  coord_fixed()
plot(g)
```

No underlying structure = no clustering / pattern.


Back to our data set of interest. We use the same process to compute the pcs, but would probably 
need to do so on a cluster with a lot of memory, and it even then it would take a long time. Instead, we'll be using tools from the package `SNPRelate` which allow us to do key computations quickly. To use `SNPRelate` functions, we need to get our data in a GDS format. `SNPRelate` has a function `snpgdsBED2GDS()` to convert PLINK binary data into a GDS file, bed/bim/fam $\longrightarrow$ GDS, but as far as I am aware, there is no tool to convert `SnpMatrix` objects to GDS, `SnpMatrix` $\centernot\longrightarrow$. So, we need 
to first convert our qc data back to bed/bim/fam using the `snpStats` function `write.plink()`, and then use that to create a GDS file: `SnpMatrix` $\longrightarrow$ bed/bim/fam $\longrightarrow$ GDS.

There is code for this in the next module (see `gwas-3.R`), but I'll include it here for completeness. I'll read in the clinical data, and re-load the qc-data because we replaced the original copy (`obj`) with the imputed version.

```{r}
clinical <- fread('data/GWAStutorial_clinical.csv')
# FIXME: Consider the following line - either the variable name or the rds being called needs to change 
imputed <- readRDS('data/gwas-qc.rds')
write.plink(
  file.base = "data/qc_data",
  snps = imputed$genotypes,
  pedigree = imputed$fam$pedigree,
  id = imputed$fam$member,
  father = imputed$fam$father,
  mother = imputed$fam$mother,
  sex = clinical$sex,
  phenotype = clinical$CAD + 1,
  chromosome = imputed$map$chromosome,
  genetic.distance = imputed$map$cM,
  position = imputed$map$position,
  allele.1 = imputed$map$allele.1,
  allele.2 = imputed$map$allele.2
)
```

If we wanted to spend the time using the mode imputation and rounding to get a qc-data
set with absolutely no missingness, we could do bed/bim/fam/ $\longrightarrow$ `SnpMatrix` bed/bim/fam/ $\longrightarrow$ GDS. This is annoying.

We will now create our GDS file, use `SNPRelate` to compute the PCs, and plot them to see if it looks like there is any kind of sample
structure as we saw in the previous example. We don't have a known race or subpopulation status vector to color this plot, but we can look for clustering.

```{r}
# create gds file so we can use SNPRelate - using unimputed qc data
qc_data.fn <- lapply(c(bed='bed', bim='bim', fam='fam', gds='gds'), function(x) paste0('./data/qc_data.', x))

snpgdsBED2GDS(qc_data.fn$bed, qc_data.fn$fam, qc_data.fn$bim, qc_data.fn$gds)

# open the file
genofile <- snpgdsOpen(qc_data.fn$gds)

# get pcs
pca <- snpgdsPCA(genofile) 

# May 09, 2022 - this snpgdsPCA() call makes R crash 

# close the file
snpgdsClose(genofile)

# plot
plot(1:10, pca$varprop[1:10], type = 'b', ylab = 'Proportion of variance explained', xlab = 'PC')


# put top 10 pcs in a table 
pctab <- data.frame(sample.id = pca$sample.id,
                    pca$eigenvect[, 1:10],
                    stringsAsFactors = FALSE)
names(pctab)[-1] <- paste0('PC', 1:10)

# plot 
plot(pctab$PC2, pctab$PC1, xlab="Principal Component 2", ylab="Principal Component 1")
```

```{r pca_base}
# attempt to do the PCA with base R functions using the imputed data 
imp_genotypes <- imp_obj$genotypes
X <- imp_genotypes@.Data
## Mean center the variables
# n <- nrow(X)
# J <- matrix(1, n, n)
# (Xtilde <- X - 1 / n * J %*% X) # manual approach to scaling 
# NB: there are R functions to do this scaling automatically 

## Compute the singular value decomposition
res <- svd(X, nu = 0, nv = 2)
# FIXME: May 12, 2022 - Error in svd(X, nu = 0, nv = 2) : infinite or missing values in 'x'
str(res)

## Compute the principal components
(Y <- X %*% res$u)

```




There is definitely some clustering in our data that is likely attributable to population
structure and/or relatedness. One way we can account for this clustering is by including the
top PCs as covariates in modeling or testing.  

Depending on the number of relevant pcs we adjust for, PCA can account for finer structure than the handfuls of large subpopulations we have seen so far. However, if we wanted to use PCA to adjust for finer population structure like family structure or cryptic relatedness, we would need to include a *lot* more pcs. Depending on our sample size, this may not be possible or how we want to spend our df. 

### Kinship / GRM / RRM 

Kinship has a colloquial interpretation that goes along with familial relatedness - people are 'kin' who are in the same family. In genetics, a more formal definition of kinship is a measure of relatedness. Formally, the probability that an allele selected randomly from an individual, $i$, and an allele selected at the same autosomal locus from another individual, $j$, are identical and from the same ancestor.

One way to measure relatedness between individuals in a GWAS context is via a measure called a **kinship coefficient** . The kinship coefficient is a probability that quantifies how likely it is that two alleles at same locus, each taken from different people, will be identical by descent. A *kinship matrix* has kinship coefficients as its elements.

### Extensions/ideas in progress -- 

```{r}
# open the file
genofile <- snpgdsOpen(qc_data.fn$gds)

# compute GRM
grm <- snpgdsGRM(genofile, method="GCTA")

# close the file
snpgdsClose(genofile)
```

* [But	what	are	genomic	
(additive)	relationships?](https://colloque.inrae.fr/iufro2016/content/download/5576/73493/version/1/file/Legarra_IUFROArcachon_2016.pdf)
* [Using the genomic relationship matrix to predict the accuracy
of genomic selection](http://genomics.cimmyt.org/SAGPDB/Slides%20Paulino/2.%20Gmatrix/accuracyGoddardHayesMeuwissen.pdf)

Calculate RRM/GRM, look for people with kinship coefficient that is greater than some threshold, remove them.

KING
PCA-Air


### Example of a LMM (forthcoming)

- show how this can be done in R
- need to read up on Anna's penalizedLMM package 

## Further reading 

 - [This other tutorial](https://smcclatchy.github.io/mapping/)
 - [Another PCA tutorial with snpStats](https://www.bioconductor.org/packages/devel/bioc/vignettes/snpStats/inst/doc/pca-vignette.pdf)



