---
title: 'GWAS tutorial: Imputation and population structure'
author: Patrick Breheny, Anna Reisetter, and Tabitha Peter
date: '`r format(Sys.Date(), "%B %d, %Y")`'
extra_dependencies: ["centernot"]
---

```{r knitr_setup, include=FALSE, purl=FALSE}
library(knitr)
library(kableExtra)
set.seed(1)
knitr::opts_knit$set(aliases=c(h = 'fig.height', w = 'fig.width'))
knitr::opts_chunk$set(comment="#", message=FALSE, collapse=TRUE, cache=FALSE, tidy=FALSE, fig.align="center")
knitr::knit_hooks$set(small.mar = function(before, options, envir) {
  if (before) par(mar = c(4, 4, .1, .1))
})
```

To begin, read in the **qc** data from earlier step (refer back to the "Data" page of the tutorial). 

```{r read}
library(snpStats)
library(SNPRelate)
library(data.table)
library(magrittr)
obj <- readRDS('data/gwas-qc.rds')
obj$genotypes
dim(obj$map)
cs <- col.summary(obj$genotypes)
```

# Imputation

In a genetics research context, most  observations (*e.g* patients or subjects) will have missing values for at least one SNP. A common method of dealing with missing SNP data is imputation. 

There are two main reasons that one would use imputation:

1. To replace missing SNP values with what these values are predicted to be, based upon a person's (or subject's) available SNP values near the loci of interest. For instance, suppose that in a given data set, patient A is missing SNP 123. This value for patient A could be imputed based on the patient's other SNP values at loci near 123. 

2. To infer values for SNPs that were not measured at all for any patients (subjects). This would be the case if one was to merge data from studies that examined different loci. For instance, suppose I am merging data from studies A and B. Suppose further that study A measured SNP 123 for all patients, but study B did not. In the merged data, I may need to impute values for SNP 123 for all patients in the study B data. 

For the purposes of this tutorial, let us limit ourselves to scenario (1). 

Recall that in the QC step of our analysis, we excluded SNPs with \ge 90\% missingness. However, there may still be SNPs with some missingness. SNPs that are not missing are described as "called." The **call rate** is the proportion of genotypes that are called (see `snpStats::col.summary()` documentation for details). Therefore, a call rate of 1 indicates that a SNP has no missing values.

By examining the call rate information, we will first check how many SNPs in our qc'd data set have some missing data:

```{r, any-missing}
table(cs$Call.rate == 1)
```

 This tells us that `r table(cs$Call.rate == 1)['TRUE']` SNPs have no missingness, but `r table(cs$Call.rate == 1)['FALSE']` still have some missingness (albeit less than 10\%.) We will try to impute values for these SNPs using the `snp.imputation()` function from `snpStats`. `snp.imputation()` has numerous options that can be tweaked according to the needs of a specific problem. We will perform a basic imputation for now; see the R documentation for more details.
 
 The package `snpStats` uses a two step imputation procedure. First, the function determines a set of "tag" SNPS. These tag SNPs are used to predict the missing SNP values and to generate prediction rules for the missing SNPs. Second, these prediction rules are applied to the supplied genotype matrix where missing SNP values are imputed. 
 
**N.B** In the case where there is insufficient data or a lack of tagging SNPs, it is possible for the generated prediction rules to fail at yielding predictions. We will see an example of this later in the tutorial. 

```{r, impute}
# determine tagging SNPs. Note: this can take a few minutes
rules <- snpStats::snp.imputation(obj$genotypes, minA=0)

# apply the prediction rules to missing SNPs and output an imputed SnpMatrix object
# NB: these imputed values will likely not be integers, so we will have to do  
# some subsequent rounding if we want to convert them back to their .bed format
imputed <- snpStats::impute.snps(rules, obj$genotypes, as.numeric=FALSE)

# how many SNPs still have missing data after imputation?
cs.imputed <- col.summary(imputed)
table(cs.imputed$Call.rate == 1)
```


In the procedure coded above, we went from `r table(cs$Call.rate == 1)['TRUE']` SNPs with no missingness to `r table(cs.imputed$Call.rate == 1)['TRUE']` after imputation. However, there are still `r table(cs.imputed$Call.rate == 1)['FALSE']` SNPs with missing data. 

We can look at the $R^2$ values to check the imputation quality. This vignette has additional information about accessing the $R^2 values and evaluating imputation quality with `snpstats`:

* [Imputation Vignette](https://www.bioconductor.org/packages/release/bioc/vignettes/snpStats/inst/doc/imputation-vignette.pdf)


Often we want our data set to have no missing values. Many statistical methods we may want to apply to these data which cannot handle any missingness. Rather than simply excluding these SNPs, one not-very-fancy, but reasonable thing to do for these values is to replace them with their HWE expected value (i.e. the average of that SNP). 

There are a number of ways to code this, but with `snpStats` it will require us to convert data from the `snpStats` `SnpMatrix` format to numeric and back. Generally, the entire `SnpMatrix` object will be too large for `R` to read into memory, so I will do this one SNP at a time in a loop. 

We can impute the mean or mode (i.e. 0, 1, or 2) of these SNPs. The mode will allow us to convert the imputed data back to PLINK file formats, but if that's our objective, we'll also have to check all of our `snpStats` imputed values and round them where necessary, which can take a while. If we don't need to interact with PLINK or PLINK-related R packages, we can leave non-integer genotype values as they are, and simply impute the remaining values using the mean. This will allow us to only work on the SNPs that have remaining missingness and will be faster, so that's what I'm going to do. 

```{r, impute_mean}
(missing <- table(cs.imputed$Call.rate == 1)) # use this to verify things are being subset appropriately, dimensions match, etc.

# identify which SNPs have missingness
to_impute <- which(cs.imputed$Call.rate < 1)
length(to_impute)

# subset to a SnpMatrix object with only SNPs with some missingness - we want to loop
# over these to replace missing values with the mean, but it's a waste of time
# to loop over the SNPs with no missingness so I'm subsetting
miss <- imputed[, to_impute]
dim(miss)

# this is done in a way where only one SNP at a time is converted to a numeric - otherwise this
# is computationally too expensive in R
imputed_mean <- sapply(1:ncol(miss), function(x){
  s <- drop(as(miss[,x], 'numeric'))
  idx <- which(is.na(s)) # identify missing values in a numeric vector
  s[idx] <- mean(s, na.rm = TRUE) # replace missing values with that SNP mean
  raw <- snpStats::mean2g(s) # convert the numeric SNP vector back to its raw, memory friendly form
  return(raw)
})
dim(imputed_mean) 

imputed2 <- imputed # I'm making a copy of the imputed data with residual missingness so I can compare the snpstats imputed data with the fully imputed data
imputed2@.Data[, to_impute] <- imputed_mean # replace the portion of the imputed matrix with missingness with the HWE-imputed version from our loop
```

Below is code to impute using the mode, and to check and convert any `snpStats` imputed values to the nearest integer values where necessary. This takes about 45 minutes (which is annoying), but may be worthwhile if your data isn't very big, or if using the PLINK formats is of interest. The overall structure is similar to the mean-imputation code above, but with some key differences. We will also have to re-check for monomorphic SNPs and potentially re-subset our data if we use this type of imputation. We also need to loop over all of the SNPs to check for non-integer values - I can't simply loop over the chunk with missingness as above.

```{r, impute_mode, eval = FALSE}
# R doesn't have a built in `mode` function so we'll have to write our own
mode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# this takes ~ 45 min
start <- Sys.time()
imputed_mode <- sapply(1:ncol(imputed), function(x){
  s <- drop(as(imputed[,x], 'numeric'))
  idx <- which(is.na(s)) # look for missing values in a numeric vector
  if (length(idx) > 0) s[idx] <- mode(s) # replace missing values with that SNP mode
  s1 <- snpStats::mean2g(s) %>%
    snpStats::g2post() # s1 contains the posterior probabilities of the possible genotypes
  to_round <- apply(s1, 1, function(z) length((z[z > 0] > 1))) 
  idx2 <- which(to_round > 1)
  s1[idx2,] <- t(apply(t(s1[idx2,, drop = FALSE]), 2, function(y) list(c(1, 0, 0),
                                                                       c(0, 1, 0),
                                                                       c(0, 0, 1))[[which.max(y)]])) # impute using the genotype with the highest posterior probability
  raw <- snpStats::mean2g(s1) # convert the numeric SNP vector back to its raw, memory friendly form
  return(raw)
})
stop <- Sys.time()
stop - start
dim(imputed_mode)

imputed3 <- as(imputed_mode, 'raw') # convert to raw

# we need to do another check for monomorphic snps with this type of imputation
cs.imputed3 <- col.summary(imputed2)
table(cs.imputed3$MAF == 0) # some of these have been imputed such that they are now monomorphic! re-subset...
```

Now back to our mean-imputed object, `imputed2`. We have a genotype object with no missingness. Lets double check that we in fact have no missing values and that our labels match, in case something weird happened that didn't throw an error. When working with large genetic data sets it is easy for something like that to occur and go unnoticed, so double checking our preprocessing is almost always worthwhile.

Again, because of the size of our `SnpMatrix` object, this isn't as simple as `sum(is.na(imputed2))`, and the following would throw an error.

```{r}
# sum(is.na(imputed2))
```

But, we can do this using the `snpStats` `col.summary()` function

```{r}
cs.imputed2 <- snpStats::col.summary(imputed2)
table(cs.imputed2$Call.rate)
```


Or, we can check missingness using base `R` functions in multiple chunks `R` can handle. The `col.summary()` method is easier and faster, but this method is more broadly applicable.

```{r}
# make sure there are no missing values remaining / count missing values
missing <- 0
chunks <- ceiling(nrow(imputed2) / 100) # I'm breaking this up using 100 based on on trial and error but this can be tweaked.
start <- 1
for (i in 1:chunks){
  stop <- min(i*100, nrow(imputed2))
  missing <- missing + sum(is.na(imputed2[start:stop,]))
  start <- stop + 1
}
missing # should be 0!
```

Now we'll replace our `obj$genotypes` with the fully-imputed version, and make sure the `map` and `fam` objects are appropriately subset and ordered.

```{r}
obj$genotypes <- imputed2
obj$map <- obj$map[colnames(obj$genotypes),]
stopifnot(all.equal(colnames(obj$genotypes), as.character(obj$map$snp.name)))
obj$fam <- obj$fam[rownames(obj$genotypes),]
stopifnot(all.equal(rownames(obj$genotypes), as.character(obj$fam$pedigree)))
```

Let's save this imputed data set for future use in downstream analyses:
```{r save}
saveRDS(obj, 'data/gwas-imp.rds')
```

# Imputation (advanced)

A more complex method of imputation involves the use of reference genome panels in addition to the observed data itself. The basic idea is to use known haplotypes, or groups of alleles inherited together, from reference genomes to give us better estimates of unobserved genotypes. These reference panels typically come from the either the 1000 Genomes project or the HapMap project, both of which are maintained by large-scale international organizations that aim to develop haplotype maps of the human genome in diverse populations. 

* [IGSR: The International Genome Sample Resource](https://www.internationalgenome.org/home)
* [International HapMap Project](https://www.genome.gov/10001688/international-hapmap-project)

In addition to allowing us to estimate untyped SNPs as we did above, where our SNPs of interest were typed in our population of interest but we had call rates of less than 1, this method of imputation can also allow us to estimate SNPs that were not genotyped on a particular population at all. This can be useful for combining multiple genetic data sets where different SNPs were typed, or for evaluating associations in distinct genetic populations. 

It's important to be aware that this type of imputation is possible, and commonly done. However, since it involves its own large array of software and expertise, it is probably something you would want to consult with an expert on. The Michigan Imputation Server is a service that will do more complex imputation for you. It also contains information about the various reference panels, their versions, etc. 

* [Michigan Imputation Server](https://imputationserver.sph.umich.edu/index.html#!)


# Population structure

## Concept

Population structure is defined by the existence of allele frequency differences that characterize sub-populations and is driven by the combined effects of evolutionary processes such as genetic drift, migration, demographic history, and natural selection.

Population structure is broadly categorized based on whether it describes recent or ancient relatedness. Ancient relatedness describes the presence of a common ancestor many generations previously. The presence of distinct ancestry groups with different allele frequencies in a sample is known as *population stratification*. Recent relatedness describes the sharing of a common ancestor only several generations previously. Pedigree-based methods may be used to explicitly model recent relatedness if familial relationships are known. In the absence of known familial relationships, recent relatedness is referred to as *cryptic relatedness*.

Population structure is a common phenomenon in genetic data. Varying levels of relatedness are almost always present among genetic samples, even in samples of unrelated individuals and seemingly homogeneous populations. For example, European American, Han Chinese, and recently, cohorts within the UK Biobank data have been shown to exhibit patterns of population and geographic structure despite their seemingly similar subjects.

Population stratification in particular has been of great concern in genetic studies due to its potential to lead to spurious associations when population structure is associated with differences in both allele frequency and the trait or disease.

As an example, consider a GWAS to assess genetic variants associated with lung cancer in a sample comprised of subjects from two distinct subpopulations, A and B. Assume the minor allele of SNP X is present with higher frequency in subpopulation A compared to subpopulation B, but has no direct effect on lung cancer. Also suppose these subpopulations are geographically segregated in a such a way that subpopulation A is exposed to good air quality, and subpopulation B to poor air quality, and that air quality has a direct effect on lung cancer. A GWAS of data from these subpopulations would find SNP X to be significantly associated with lung cancer, even though we know it has no effect on lung cancer. If subpopulations A and B were not subject to different air qualities, all else being equal, SNP X would not be found to be associated with the phenotype.

Another apocryphal but illustrative example of how population stratification can lead to confounding in genetic studies is linked below.

* [Beware the chopsticks gene](https://www.nature.com/articles/4000662)

In addition, GWAS (and many statistical tests) assume samples are independent. Cryptic relatedness and population structure can invalidate these tests since the presence of non-independent samples, and thus non-independent errors, can lead to inflated test statistics. 

Because of this, it is critical to evaluate and account for potential population structure in our data in order to avoid false positives and negatives. 
 
```{r, include=FALSE}
obj <- readRDS('data/gwas-imp.rds')
```
 

## PCA

One of the simplest and most common methods used to assess and correct for population structure in our data is with PCA. Conceptually, PCA can be thought of as extracting the axes of greatest variability in our data. Mathematically, this entails using the eigenvectors of the sample covariance matrix, $\frac{1}{n}\bf{X}^T\bf{X}$ (assuming the columns of $\bf{X}$ are centered), or in the case of population structure, since we are interested in variation across samples rather than features, $\frac{1}{p}\bf{X}\bf{X}^T$. ... also more mathematical details that I will include ... eventually.

Our data will require special tools for PCA because of its size. We'll get there, but to start, I'm going to illustrate some of these concepts using some smaller data with known structure. First, w'll read in the small data and filter out the monomorphic SNPs. Note that this data set contains a known race variable.
```{r, process_admixture}
Data <- read.delim("https://s3.amazonaws.com/pbreheny-data-sets/admixture.txt")
Data[1:5, 1:5]
Race <- Data$Race
XX <- as.matrix(Data[,-1]) # create an all SNP mat
polymorphic <- apply(XX, 2, sd) != 0
X <- XX[,polymorphic] # filter out monomorphic SNPs
dim(X)
# [1] 197  98
table(Race)
```
Next, I'm going to compute the pcs and plot a scree plot. This tells us the proportion of variance explained by each of the pcs. PCA is such that the pcs are ordered from those that explain the greatest to least amount of variability. Scree plots are sometimes used to decide how many pcs are appropriate to include. We look for the 'elbow' in the plot, which indicates when the proportion of variance explained by including additional pcs may not be worth the extra df. There are also a number of more complex tests and tools to determine this as well, or often the top 10 pcs are simply used in practice. 
```{r, plot_admixture}
pca <- prcomp(X, center = TRUE, scale = TRUE)
pca$x[1:5, 1:5]
# I'm only plotting the top 10 pcs
plot(1:10, pca$sdev[1:10]^2, type = 'b', ylab = 'Proportion of variance explained', xlab = 'PC', main = 'Scree Plot')

```

We see this elbow point at 3 pcs, which makes sense, given that there are 4 distinct races in this data set (we lose one df when we center our data).

Now we'll plot the first two pcs against each other, and color the data points by the known race of each subject. We expect to see clustering that corresponds to population structure. 

```{r}
gdat <- data.frame(Race = Race, PC1 = pca$x[,1], PC2 = pca$x[, 2])
library(ggplot2)
g <- ggplot(gdat, aes(x = PC1, y = PC2, col = Race)) +
  geom_point() +
  coord_fixed()
plot(g)
```

Indeed, we see that the first two pcs differentiate the racial groups, which cluster together. We can see that PC1 seems to differentiate the European/Japanese populations from the African/African American ones, while PC2 seems to primarily differentiate the European and Japanese populations. Even if we did not have the known race vector to color the points, we could still pick out some clustering from this plot, which indicates underlying structure. If there were no underlying structure in our data, we would expect to see no clustering or systematic pattern in this plot. 

As a counter example to this plot, we can plot the pcs from a random matrix:

```{r}
# I'm going to use the same dimensions as those in the admixture data.
n <- 197
p <- 98
XX <- matrix(rnorm(n * p), n, p)
pca2 <- prcomp(XX, center = TRUE, scale = TRUE)
gdat2 <- data.frame(PC1 = pca2$x[,1], PC2 = pca2$x[, 2])
g <- ggplot(gdat2, aes(x = PC1, y = PC2)) +
  geom_point() +
  coord_fixed()
plot(g)
```

No underlying structure = no clustering / pattern.


Back to our data set of interest. We use the same process to compute the pcs, but would probably 
need to do so on a cluster with a lot of memory, and it even then it would take a long time. Instead, we'll be using tools from the package `SNPRelate` which allow us to do key computations quickly. To use `SNPRelate` functions, we need to get our data in a GDS format. `SNPRelate` has a function `snpgdsBED2GDS()` to convert PLINK binary data into a GDS file, bed/bim/fam $\longrightarrow$ GDS, but as far as I am aware, there is no tool to convert `SnpMatrix` objects to GDS, `SnpMatrix` $\centernot\longrightarrow$. So, we need 
to first convert our qc data back to bed/bim/fam using the `snpStats` function `write.plink()`, and then use that to create a GDS file: `SnpMatrix` $\longrightarrow$ bed/bim/fam $\longrightarrow$ GDS.

There is code for this in gwas-3, but I'll include it here for completeness. I'll read in the clinical data, and re-load the qc-data because we replaced the original copy (`obj`) with the imputed version.
```{r}
clinical <- fread('data/GWAStutorial_clinical.csv')
imputed <- readRDS('data/gwas-qc.rds')
write.plink(
  file.base = "data/qc_data",
  snps = imputed$genotypes,
  pedigree = imputed$fam$pedigree,
  id = imputed$fam$member,
  father = imputed$fam$father,
  mother = imputed$fam$mother,
  sex = clinical$sex,
  phenotype = clinical$CAD + 1,
  chromosome = imputed$map$chromosome,
  genetic.distance = imputed$map$cM,
  position = imputed$map$position,
  allele.1 = imputed$map$allele.1,
  allele.2 = imputed$map$allele.2
)
```

If we wanted to spend the time using the mode imputation and rounding to get a qc-data
set with absolutely no missingness, we could do bed/bim/fam/ $\longrightarrow$ `SnpMatrix` bed/bim/fam/ $\longrightarrow$ GDS. This is annoying.

We will now create our GDS file, use `SNPRelate` to compute the PCs, and plot them to see if it looks like there is any kind of sample
structure as we saw in the previous example. We don't have a known race or subpopulation status vector to color this plot, but we can look for clustering.

```{r}
# create gds file so we can use SNPRelate - using unimputed qc data
qc_data.fn <- lapply(c(bed='bed', bim='bim', fam='fam', gds='gds'), function(x) paste0('./data/qc_data.', x))
snpgdsBED2GDS(qc_data.fn$bed, qc_data.fn$fam, qc_data.fn$bim, qc_data.fn$gds)

# open the file
genofile <- snpgdsOpen(qc_data.fn$gds)

# get pcs
pca <- snpgdsPCA(genofile)

# close the file
snpgdsClose(genofile)

# plot
plot(1:10, pca$varprop[1:10], type = 'b', ylab = 'Proportion of variance explained', xlab = 'PC')


# put top 10 pcs in a table 
pctab <- data.frame(sample.id = pca$sample.id,
                    pca$eigenvect[, 1:10],
                    stringsAsFactors = FALSE)
names(pctab)[-1] <- paste0('PC', 1:10)

# plot 
plot(pctab$PC2, pctab$PC1, xlab="Principal Component 2", ylab="Principal Component 1")
```

There is definitely some clustering in our data that is likely attributable to population
structure and/or relatedness. One way we can account for this clustering is by including the
top PCs as covariates in modeling or testing.  

Depending on the number of relevant pcs we adjust for, PCA can account for finer structure than the handfuls of large subpopulations we have seen so far. However, if we wanted to use PCA to adjust for finer population structure like family structure or cryptic relatedness, we would need to include a *lot* more pcs. Depending on our sample size, this may not be possible or how we want to spend our df. 

## Kinship / GRM / RRM 

Kinship has a colloquial interpretation that goes along with familial relatedness - people are 'kin' who are in the same family. In genetics, a more formal definition of kinship is a measure of relatedness. Formally, the probability that an allele selected randomly from an individual, $i$, and an allele selected at the same autosomal locus from another individual, $j$, are identical and from the same ancestor.

## Cyptic relatedness

```{r}
# open the file
genofile <- snpgdsOpen(qc_data.fn$gds)

# compute GRM
grm <- snpgdsGRM(genofile, method="GCTA")

# close the file
snpgdsClose(genofile)
```

* [But	what	are	genomic	
(additive)	relationships?](https://colloque.inrae.fr/iufro2016/content/download/5576/73493/version/1/file/Legarra_IUFROArcachon_2016.pdf)
* [Using the genomic relationship matrix to predict the accuracy
of genomic selection](http://genomics.cimmyt.org/SAGPDB/Slides%20Paulino/2.%20Gmatrix/accuracyGoddardHayesMeuwissen.pdf)

Calculate RRM/GRM, look for people with kinship coefficient that is greater than some threshold, remove them.

KING
PCA-Air

NB: definitions of the terms `recent` and `ancient` are somewhat subjective and hand-wavy since, in theory, if you look back far enough, everyone shares a common ancestor. However, the idea is that humans migrated, separated, and mated such that over time distinct groups developed allele frequencies different enough to confound an analysis.






