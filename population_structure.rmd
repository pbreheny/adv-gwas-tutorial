---
title: 'GWAS tutorial: Population structure'
author: Patrick Breheny, Anna Reisetter, and Tabitha Peter
date: '`r format(Sys.Date(), "%B %d, %Y")`'
extra_dependencies: ["centernot"]
editor_options: 
  chunk_output_type: console
---

```{r knitr_setup, include=FALSE, purl=FALSE}
library(knitr)
library(kableExtra)
set.seed(1)
knitr::opts_knit$set(aliases=c(h = 'fig.height', w = 'fig.width'))
knitr::opts_chunk$set(comment="#", message=FALSE, collapse=TRUE, cache=FALSE, tidy=FALSE, fig.align="center")
knitr::knit_hooks$set(small.mar = function(before, options, envir) {
  if (before) par(mar = c(4, 4, .1, .1))
})
```

# Before you begin...

## Motivating questions 

1. What assumptions does a GWAS make about a population? 

2. What happens when these assumptions are not met? 

## Objectives of this module 

When you have completed this module, you will know: 

1. ... how to summarize the assumptions (biological and statistical) that a GWAS makes about the individuals in a study and the structure of the data. 

2. ... how to identify instances when your data show evidence of **not** meeting these assumptions. 

3. ... what tools are available for analyzing complex data that do not meet the basic GWAS assumptions. 

4. ... how to implement the tools mentioned above using `R`

## Set up 

The starting point for the computation in this module depends on what you did in the previous module (imputation). If your data was too big to read into your computer's memory, read the section labeled "PCA for large/not fully imputed data" for your computational methods. If you were able to work with a matrix of all your SNP data, refer to the "PCA for fully-imputed data" for computational methods.   

Remember: "fully imputed" means that there are no missing values in the data set. 

# Population structure

## Concept

In a GWAS context, *population structure* is a kind of relatedness among the individuals represented in the data set. When considering relatedness, it is helpful to define the phrase **identical by descent**. Two alleles from the same locus are identical by descent if they can be traced back from the same allele in an earlier generation. For more information on this concept, check out [this example](https://pbgworks.org/sites/pbgworks.org/files/KinshipMatrixFinal.pdf). 


Population structure is defined by the existence of allele frequency differences that characterize sub-populations and is driven by the combined effects of evolutionary processes such as genetic drift, migration, demographic history, and natural selection.

Population structure is a common phenomenon in genetic data. Varying levels of relatedness are almost always present among genetic samples, even in samples of unrelated individuals and seemingly homogeneous populations. For example, European American, Han Chinese, and recently, cohorts within the UK Biobank data have been shown to exhibit patterns of population and geographic structure despite their seemingly similar subjects.

Population structure is broadly categorized based on whether it describes recent or ancient relatedness. Ancient relatedness describes the presence of a common ancestor many generations previously. The presence of distinct ancestry groups with different allele frequencies in a sample is known as *population stratification* (a.k.a *ancestry differences*). Recent relatedness describes the sharing of a common ancestor only several generations previously. Pedigree-based methods may be used to explicitly model recent relatedness if familial relationships are known. In the absence of known familial relationships, recent relatedness is referred to as *cryptic relatedness*. ^[NB: definitions of the terms `recent` and `ancient` are somewhat subjective and hand-wavy since, in theory, if you look back far enough, everyone shares a common ancestor. However, the idea is that humans migrated, separated, and mated such that over time distinct groups developed allele frequencies different enough to confound an analysis.]

## Population stratification 

Population stratification in particular has been of great concern in genetic studies due to its potential to lead to spurious associations when population structure is associated with differences in both allele frequency and the trait or disease.

As an example, consider a GWAS to assess genetic variants associated with lung cancer in a sample comprised of subjects from two distinct subpopulations, A and B. Assume the minor allele of SNP X is present with higher frequency in subpopulation A compared to subpopulation B, but has no direct effect on lung cancer. Also suppose these subpopulations are geographically segregated in a such a way that subpopulation A is exposed to good air quality, and subpopulation B to poor air quality, and that air quality has a direct effect on lung cancer. A GWAS of data from these subpopulations would find SNP X to be significantly associated with lung cancer, even though we know it has no effect on lung cancer. If subpopulations A and B were not subject to different air qualities, all else being equal, SNP X would not be found to be associated with the phenotype.

Another apocryphal but illustrative example of how population stratification can lead to confounding in genetic studies is linked below.

* [Beware the chopsticks gene](https://www.nature.com/articles/4000662)

## Cryptic relatedness 

An example of cryptic relatedness would be an instance where, unbeknownst to the researcher(s), some of the individuals in a study are first cousins. 

## Tools/implementation 

GWAS (and many statistical tests) assume samples are independent. Cryptic relatedness and population structure can invalidate these tests since the presence of non-independent samples, and thus non-independent errors, can lead to inflated test statistics. 

With this in mind, it is critical to evaluate and account for potential population structure in our data in order to avoid false positives and negatives. 
 
 
## Principle component analysis (PCA) - brief introduction
 
 One of the simplest and most common methods used to assess and correct for population structure in our data is with principle component analysis (PCA). Here, I will present a 'crash course' on the concepts of PCA and provide an example.

  Conceptually, PCA can be thought of as extracting the axes of greatest variability in our data. PCA creates linear combinations of a given set of variables; this lets the user represent a lot of correlated variables with a smaller number of uncorrelated variables - so PCA gives us **dimension reduction**, which is really helpful for high dimensional settings like GWAS data.
  
  Connections to different conceptual frameworks:

  - From a linear modeling framework, PCA is an orthogonal linear transformation of a data set.

  - From a machine learning framework, PCA is a type of unsupervised learning. 
  
  A typical PCA includes the following steps: 

  1. Standardize (center and scale) the data.  
  
  2. Calculate the covariance matrix $\textbf{S} = \tilde{X}^T \tilde{X}$, where $\tilde{X}$ is our centered and scaled design matrix from step (1). 
  
  3. Find the eigenvectors and eigenvalues of $\textbf{S}$
  
  4. Calculate the inner products of the columns of $\tilde{X}$ and the associated eigenvectors - these inner products are the **principal components**, whereas the eigenvector coefficients are the **principal component loadings**
  
  5. Determine a number of principal components to keep, and create a feature vector with only these components
  
  6. Plot the feature vector along the principal component axes (as opposed to the axes of the original data space) - these plots can reveal clusters/population structures within the data 

  
Our data will require special tools for PCA because of its size. We'll get there, but to start, I'm going to illustrate some of these concepts using some smaller data with known structure. First, we'll read in the small data and filter out the monomorphic SNPs. Note that this data set contains a known race variable.

```{r, process_admixture}
Data <- read.delim("https://s3.amazonaws.com/pbreheny-data-sets/admixture.txt")
Data[1:5, 1:5]
Race <- Data$Race
XX <- as.matrix(Data[,-1]) # create an all SNP mat
polymorphic <- apply(XX, 2, sd) != 0
X <- XX[,polymorphic] # filter out monomorphic SNPs
dim(X)
# [1] 197  98
table(Race)
```

Next, I'm going to compute the principal components (PCs) and plot a scree plot. This tells us the proportion of variance explained by each of the PCs. PCA is such that the PCs are ordered from those that explain the greatest to least amount of variability. Scree plots are sometimes used to decide how many PCs are appropriate to include. We look for the 'elbow' in the plot, which indicates when the proportion of variance explained by including additional PCs may not be worth the extra df. There are also a number of more complex tests and tools to determine this as well, or often the top 10 PCs are simply used in practice. 
```{r, plot_admixture}
pca <- prcomp(X, center = TRUE, scale = TRUE)
pca$x[1:5, 1:5]
# I'm only plotting the top 10 PCs
plot(1:10, pca$sdev[1:10]^2, type = 'b', ylab = 'Proportion of variance explained', xlab = 'PC', main = 'Scree Plot')

```

We see this elbow point at 3 PCs, which makes sense, given that there are 4 distinct races in this data set (we lose one degree of freedom when we center our data).

Now we'll plot the first two PCs against each other, and color the data points by the known race of each subject. We expect to see clustering that corresponds to population structure. 

```{r}
gdat <- data.frame(Race = Race, PC1 = pca$x[,1], PC2 = pca$x[, 2])
library(ggplot2)
g <- ggplot(gdat, aes(x = PC1, y = PC2, col = Race)) +
  geom_point() +
  coord_fixed()
plot(g)
```

Indeed, we see that the first two PCs differentiate the racial groups, which cluster together. We can see that PC1 seems to differentiate the European/Japanese populations from the African/African American ones, while PC2 seems to primarily differentiate the European and Japanese populations. Even if we did not have the known race vector to color the points, we could still pick out some clustering from this plot, which indicates underlying structure. If there were no underlying structure in our data, we would expect to see no clustering or systematic pattern in this plot. 

As a counter example to this plot, we can plot the PCs from a random matrix:

```{r}
# I'm going to use the same dimensions as those in the admixture data.
n <- 197
p <- 98
XX <- matrix(rnorm(n * p), n, p)
pca2 <- prcomp(XX, center = TRUE, scale = TRUE)
gdat2 <- data.frame(PC1 = pca2$x[,1], PC2 = pca2$x[, 2])
g <- ggplot(gdat2, aes(x = PC1, y = PC2)) +
  geom_point() +
  coord_fixed()
plot(g)
```

No underlying structure = no clustering / pattern.

## PCA for large/not fully imputed data

Still working on this... 


## PCA for fully-imputed data

If you have saved the fully imputed data set, load it here: 

```{r load_X}
X <- readRDS(file = "data/fully_imputed_numeric.rds")
```

In this case, we can use base `R` functions to accomplish a PCA. For our illustration, suppose we are only interested in the first two PCs. 

### using base `R` functions 
```{r pca_base, eval=FALSE}

# Center & scale X 
X <- scale(X) # the same
## Compute the singular value decomposition on the centered & scaled X 
res <- svd(t(X_tilde), nu = 0, nv = 2)
# nu = 0 indicates that I am not interested in computing left-singular vectors - these are not needed for PCA 
# nv = 2 indicates that I am interested in only the first two PCs

## Compute the principal components
(pca <- X_tilde %*% res$u)[1:2]

```

### Using another appraoch

  At the time of this writing, the code above makes my `R` session crash. 
As an alternative approaches, I can center and scale the matrix in "chunks", instead of trying to scale all of $X$ at once. 


```{r pca_alternative, eval=FALSE}
# determine the size of the "chunks" 
# remember, we are centering & scaling the columns (SNPs)
batch_size <- 1000

batches <- ceiling(ncol(X) / batch_size) 

# create a scaled matrix
X_scaled <- matrix(nrow = nrow(X), ncol = ncol(X))
# This take a while - recommend using a progress bar 
pb <- txtProgressBar(min = 0, max = batches, initial = 0) 
# start the loop 
for (b in 1:batches){
  
  start <- b*batch_size - (batch_size - 1)
  stop <- ifelse(b*batch_size > ncol(X_scaled),
                 ncol(X_scaled),
                 b*batch_size)
  
  # center and scale columns in batch b 
  X_scaled[,start:stop] <- scale(X[,start:stop])
  
  # update progress bar
  setTxtProgressBar(pb,b)
}

## Compute the singular value decomposition on the centered & scaled X 
# pca <- svd(t(X_scaled), nu = 0, nv = 2)
# nu = 0 indicates that I am not interested in computing left-singular vectors - these are not needed for PCA 
# nv = 2 indicates that I am interested in only the first two PCs


# FIXME: this makes R crash as of May 24, 2022 

```

```{r pca_take_3}

library(rsvd) # Paackage for randomized singular value decomposition 

pca <- rpca(A = X, # the matrix of SNP data
            k = 2, # the number of PCs to be computed
            center = TRUE, # center the data
            scale = TRUE, # scale the data 
            rand = TRUE # using randomized SVD will speed things up 
            )

```

