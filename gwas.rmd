---
title: GWAS tutorial
author: Patrick Breheny
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    toc: true
    toc_float: true
---

```{r knitr_setup, include=FALSE, purl=FALSE}
library(knitr)
library(kableExtra)
set.seed(1)
knitr::opts_knit$set(aliases=c(h = 'fig.height', w = 'fig.width'))
knitr::opts_chunk$set(comment="#", message=FALSE, collapse=TRUE, cache=FALSE, tidy=FALSE, fig.align="center")
knitr::knit_hooks$set(small.mar = function(before, options, envir) {
  if (before) par(mar = c(4, 4, .1, .1))
})
```
```{r setup, include=FALSE}
library(data.table)
library(magrittr)
```

Throughout, I'm going to use the `data.table` package to read in and work with data frames; feel free to use something else.  With one exception, the files are just white space delimited text files, anything can open them.

**IN PROGRESS**

# Getting the data

We will be using publicly available GWAS data from the PennCATH study of genetic risk factors for coronary artery disease.

* [Paper](http://www.ncbi.nlm.nih.gov/pubmed/21239051)

* [Data (zip)](https://www.mtholyoke.edu/courses/afoulkes/Data/GWAStutorial/GWASTutorial_Files.zip)

Download and unzip the data; you can read the paper as well if you wish.  In what follows, I will assume that the unzipped data files are in a folder called `data`; if you store them somewhere else, change the directory references.

# File formats

The data are given in "PLINK" format, which is the most common format for chip-based GWAS data (as of this writing!).  `PLINK` is an open-source whole genome association analysis toolset designed to perform a range of basic large-scale analyses in a computationally efficient manner.  It is worth knowing how to use PLINK, although you can also do most of these things in R.

I'll discuss PLINK the software program later on; for now, I'll just describe the organization of its files.

Among the zipped files are three that are necessary to perform a GWAS, the `.bed`, `.bim`, and `.fam` files.  

## `.fam`

This contains information on the subjects:

```{r fam}
(fam <- fread('data/GWAStutorial.fam'))
```

There are 1401 rows, one for each subject.  The six colums are:

1. Family ID
2. Individual ID
3. Paternal ID
4. Maternal ID
5. Sex (1=male; 2=female; other=unknown)
6. Phenotype

In this data set, columns 2-4 are unimportant.  In general, they are used to specify pedigrees (e.g., subject 3 is the daughter of subjects 1 and 2).  In this study, however, none of the subjects are related, so the only column that is important is the first, which records the subject's unique ID.

Phenotype is typically used to record case-control status or something like that, but it is also quite common to just record clinical/biological information in a separate spreadsheet, which is what was done here.

```{r clinical}
(clinical <- fread('data/GWAStutorial_clinical.csv'))
```

As you can see, we've got the `FamID` to match this spreadsheet up with the genetic data, the disease status (`CAD=1` means that the subject has coronary artery disease), and some covariates (age, triglycerides, HDL and LDL cholesterol levels).

## `.bim`

The `.bim` file, by contrast, contains information on the genetic loci (SNPs):

```{r bim}
(bim <- fread('data/GWAStutorial.bim'))
```

As you can see, we have `r nrow(bim)` rows here, one for each SNP measured in the study.  The columns are:

1. chromosome (1-22, X, Y or 0 if unplaced)
2. rs# or snp identifier
3. Genetic distance (morgans)
4. Base-pair position (bp units)
5. Allele 1 (usually minor)
6. Allele 2 (usually major)

It is pretty common for column 3 to be ignored, as it is here.

So, for example, the file tells us that genetic locus rs12565286 is located 721290 bases into chromosome 1, and that most people have a C there, but some have a G.

## `.bed`

Finally, the `.bed` file, which has all the data.  This is by far the largest of the three files, as it contains the entire `r nrow(fam)` by `r nrow(bim)` matrix of genotype calls for every subject and every locus.  To keep things manageable, this file is encoded in a special binary format -- i.e., you can't just read it in through normal means.

To access it, you'll have to use specialized applications.  I'll discuss two, an R package called `snpStats` and a command-line interface (CLI) called PLINK.

# Software

## snpStats

This is a Bioconductor package.  So, you'll have to install it via `BiocManager`

```{r}
# install.packages('BiocManager')
# BiocManager::install('snpStats')
library(snpStats)
```

To read in data, there is the `read.plink()` function:

```{r read-plink}
obj <- read.plink('data/GWAStutorial')
```

The function assumes that all the files have the same base filename, and differ only in their extension.  If this is not the case, then you need to specify the filenames for the `.bed`, `.bim`, and `.fam` files separately.

From here, `snpStats` has a lot of functions.  For example, here's a plot (there are 1401 points, one for each subject) of whether the call rate (% of genotype calls that are non-missing) is related to the heterozygosity rate (% of loci that are called AB, as opposed to AA or BB):

```{r snp-summary}
plot(row.summary(obj$genotypes)[c(1,3)])
```

Feel free to read the `snpStats` documentation and explore for yourself, but one standard thing that one is always interested in is to simply convert various SNPs to a regular numeric matrix so that you can analyze them using standard R tools.  For example, let's do a Fisher's exact test to see whether CAD is associated with SNP 143:

```{r convert-numeric}
x <- as(obj$genotypes[,143], 'numeric')
fisher.test(drop(x), clinical$CAD)
```

A GWAS is then basically just a big loop where we repeat this analysis for every single SNP (although there are of course statistical issues that come up in doing so).

Side note: In general, code like the above is risky, as it assumes that the clinical spreadsheet is in the same order as the `.fam` and `.bed` files.  This happens to be the case here:


```{r check}
all.equal(rownames(x), as.character(clinical$FamID))
```

But you should get in the habit of explicitly checking for things like this by including lines like this in your code:

```{r check-test}
stopifnot(all.equal(rownames(obj$genotypes), as.character(clinical$FamID)))
```

This will trigger an error if the condition is not met, and do nothing otherwise.

I'll write more for next week....

## PLINK

NOTE: This is incomplete.  Hopefully I'll add to it later, but in the meantime, I'm just pointing you to the (rather good) PLINK documentation and hoping you can figure things out on your own!

There are three versions of PLINK (in order from old-and-stable to new-and-fast-but-sometimes-slightly-incomplete):

* [PLINK Homepage (1.07)](http://zzz.bwh.harvard.edu/plink)

* [PLINK Homepage (1.9)](https://www.cog-genomics.org/plink2)

* [PLINK Homepage (2.0)](https://www.cog-genomics.org/plink2.0)

As an independent learning assignment, it would be good to familiarize yourself with how PLINK works and to carry out some of the same analyses in R and PLINK.  In particular, there are a few things that I think are easier in PLINK than R (or maybe they exist in some package I'm not aware of), so a useful skill is to be able to do some analysis in PLINK but then read that data back into R and merge it in with the rest of an analysis.

# Quality control

The first step in any GWAS is to examine the data for potential problems.  You don't want to carry out a GWAS, think you have an exciting result, then discover that it was all just an artifact of bad data.  This is a fairly "clean" data set, so it's not really ideal for showing these steps, but I'll go through them anyway.  There is also a sample data set in the `snpStats` package with some (at least one) bad samples; might be worth checking that one out as well.

```{r snpstats-data}
data(for.exercise)
snps.10
```

Most of these QC steps involve calculating summaries at the individual ("row") level or the SNP ("column") level:

```{r summaries}
rs <- row.summary(obj$genotypes)
cs <- col.summary(obj$genotypes)
ggbox <- function (X, xlab = "ind", ylab = "values") {
    if (!is.data.frame(X)) X <- as.data.frame(X)
    ggplot2::ggplot(utils::stack(X), ggplot2::aes_string("ind", 
        "values")) + ggplot2::geom_boxplot() + ggplot2::xlab(xlab) + 
        ggplot2::ylab(ylab)
}
```

I'm also defining a little shortcut function for box plots.

## Missing data

Any SNP with a lot of missing data is probably questionable; these SNPs are often excluded from analysis (although we will talk about other approaches later).  Likewise, any sample with lots of missing data suggests that there may be issues with the processing of that sample.

```{r miss, h=4, w=4, out.width='50%', fig.align='default', fig.show='hold'}
ggbox(rs$Call.rate, 'Individuals', 'Call rate')
ggbox(cs$Call.rate, 'SNPs', 'Call rate')
```

Individuals look good -- SNPs, on the other hand, there are definitely some SNPs with lots of missing values.  A common practice is to exclude SNPs with >5% missing data.  You could do this via:

```{r subset-1}
# (obj$genotypes <- obj$genotypes[, cs$Call.rate > 0.95])
# obj$map <- obj$map[cs$Call.rate > 0.95, ]
```

However, I'm commenting these lines out for now, since if we subset the data at this point, it won't match the column summaries, etc.

### Imputation

A common method of dealing with SNPs with missing data is imputation. This involves replacing missing SNP values with what these values are predicted to be, based on a subjects' surrounding SNP values that are not missing. 

We can first check how many SNPs have any missing data:
```{r, any-miss}
table(cs$Call.rate == 1)
```
This tells us that 172633 SNPs have no missingness, while 688840 have some level of missingness. We will try to impute values for these SNPs using the `snp.imputation()` function from `snpStats`. `snp.imputation()` has many options and things that can be tweaked. We will perform a basic imputation for now, but see its documentation for more details. `snpStats` uses a two step imputation procedure where we first determines a set of "tag" SNPS, which will be used to help predict the missing SNP values. These "tag" SNPs are then used to generate prediction rules for the missing SNPs. Then, these prediction rules are applied to our genotype matrix and missing SNP values are imputed. It is possible for the rules to not yield a predictions for SNPs with insufficient data or tagging SNPs as we'll see.

```{r, impute}
# determine tagging SNPs. Note: this can take a few minutes
rules <- snp.imputation(obj$genotypes, minA=0) 

# apply the prediction rules to missing SNPs and output an imputed SnpMatrix object
imputed <- impute.snps(rules, obj$genotypes, as.numeric=FALSE)

# how many SNPs still have missing data after imputation?
cs.imputed <- col.summary(imputed)
table(cs.imputed$Call.rate == 1)
```
Even after imputation, there are still 305651 SNPs with missing data. If a large proportion of these SNPs are missing, we may wish to exclude them.

```{r}
# how many SNPs cannot be imputed and still have >= 50% missing values?
table(cs.imputed$Call.rate <= 0.5)
# these look weird I think because there are non-polygenic snps and maybe nonstandard chr included...

# looking at the map file there are a lot of NA alleles (?)
# > sum(is.na(obj$map$allele.1))
# [1] 26154
# > sum(is.na(obj$map$allele.2))
# [1] 1

# throw out SNPs that have >= 50% missingness, even after imputation
# imputed2 <- imputed[, cs.imputed$Call.rate > 0.5]

# impute remaining missing values with HWE expected value (SNP mean)
# missing <- table(snpStats::col.summary(out2)$Call.rate == 1)

# which SNPs have missingness
# to_impute <- which(cs.imputed$Call.rate < 1)

# subset to a SnpMatrix with only SNPs with some missingness - we want to loop
# over these to replace missing values with the mean, but it's a waste of time
# to loop over the SNPs with no missingness 
# miss <- imputed[, to_impute]

# this is done in a way where only one SNP at a time is converted to a numeric - otherwise this
# is computationally too expensive in R
# imputed_mean <- sapply(1:ncol(miss), function(x){
#   s <- drop(as(miss[,x], 'numeric'))
#   if (all(is.na(s))) {
#     print(x)
#     return(miss[,x])
#   } else {
#     idx <- which(is.na(s)) # look for missing values in a numeric vector
#     s[idx] <- mean(s, na.rm = TRUE) # replace missing values with that SNP's mean
#     raw <- snpStats::mean2g(s) # convert the numeric SNP vector back to its raw, memory friendly form
#   return(raw)
#   }
# })
# 
# imputed3 <- imputed2
# imputed3@.Data[, to_impute] <- imputed_mean
```



## Sex check

Since we have genetic data on these individuals, including the X chromosome, we can determine (or at least, estimate) their "genetic sex" and compare that to the sex that is recorded in their clinical information.  A discrepancy is very troubling, as it may be the result of a sample being switched or mis-labeled (there are other explanations as well).

I am not aware of a function in `snpStats` that does this, so you'll have to use PLINK for this.  The relevant command is called `--check-sex`.

So, your assignment:

1. Run the `--check-sex` command in PLINK (this will generate an output file)
2. Read that file back into R
3. Check to see if there are any mismatches between the sex recorded in `clinical.csv` and the genetically determined sex.

You'll need to read the relevant documentation for this; for example, [here's the 1.9 documentation](https://www.cog-genomics.org/plink/1.9/basic_stats#check_sex).

Note that there are no discepancies between the sex recorded in `clinical.csv` and the one recorded in the `.fam` file:

```{r check-sex}
table(obj$fam$sex, clinical$sex)
```

## Minor allele frequency

Minor allele frequency is the percent of alleles that belong to less common category.  For example:

```{r maf-example}
(Tab <- table(as(obj$genotypes[,143], 'numeric')))
(2*Tab[1] + Tab[2]) / (2*sum(Tab))
cs[143,]$MAF
```

Excluding SNPs on the basis of minor allele frequency is a bit controversial.  It's done, and it makes sense, but has nothing to do with quality control -- there is no reason to think there are any errors in the data.  The main justification is statistical:

* If MAF is low, power is low (i.e., don't spend multiple testing corrections on tests that are unlikely to find anything anyway)
* Some statistical methods perform badly with low MAF (e.g., the $chi^2$-test)

An appropriate cutoff definitely depends on sample size -- the larger the sample, the greater your ability to include rare SNPs.  Let's look at the distributon of MAFs:

```{r maf}
hist(cs$MAF, breaks=seq(0, 0.5, 0.01), border='white', col='gray', las=1)
```

With a sample size of `r nrow(obj$genotypes)`, I would say a reasonable MAF would be something like 0.001 (0.1%).

```{r maf-exclude}
# How many SNPs would this exclude?
table(cs$MAF < 0.001)
# Would we really learn anything from analyzing a SNP like this?
table(as(obj$genotypes[,62], 'numeric'))
```

## Hardy-Weinberg equilibrium

The [Hardy-Weinberg principle](https://en.wikipedia.org/wiki/Hardy-Weinberg_principle) states that under the assumption of random mating, the distribution of genotypes should follow a binomial distribution with probability $\pi$ equal to the MAF.  If this doesn't happen, this is an indication that either:

1. There was a genotyping error for this SNP, or
2. Mating is not random

In the real world, mating is of course not random, making it difficult to exclude SNPs on the basis of HWE.  The usual recommendation is to exclude a SNP only if HWE is hugely violated (e.g., $p < 10^{-10}$ for a test of whether the data follow a binomial distribution).

```{r hwe}
ggbox(cs$z.HWE)  # Mostly near zero, but some huge outliers
p_hwe <- 2*pnorm(-abs(cs$z.HWE))
table(p_hwe < 10^(-10))
# This seems utterly bizarre -- why would there be so many A/B's, but
# no A/A's or B/B's?  Something is definitely wrong:
table(as(obj$genotypes[,which.max(cs$z.HWE)], 'character'))
```

<!-- ## Polygenic and chromosome checks -->

<!-- Another element of the SNP ("column") level summaries to pay attention to are the columns -->

<!-- * P.AA (proportion of subjects homozygous for Allele 1) -->
<!-- * P.AB (proportion of subjects heterozygous) -->
<!-- * P.BB (proportion of subjects homozygous for Allele 2) -->

<!-- In general, only SNPs that have some level of variability will be informative for analysis. For example, if some subjects have a condition and others don't, but they all have the same genotype for -->
<!-- a particular SNP, that SNP isn't going to give us any information about that condition or trait. -->

<!-- Only keep polygenic SNPs and those on chr1:22 -->
<!--   genotypes <- obj$genotypes[, snpStats::col.summary(obj$genotypes)$P.AA != 1 & -->
<!--                                snpStats::col.summary(obj$genotypes)$P.AB != 1 & -->
<!--                                snpStats::col.summary(obj$genotypes)$P.BB != 1 & obj$map$chromosome %in% 1:22] -->


## Heterozygosity

A somewhat similar idea, but applied to individuals instead of SNPs (if an individual had a ton of A/B calls but no A/A or B/B calls, or vice versa, that would indicate something was wrong):

```{r zygosity}
ggbox(rs$Heterozygosity)
```

No big outliers here, though.

## Relatedness

Another common QC check that people apply is to see whether anyone in their data set is related to each other (i.e., their genomes are far more similar than the genomes of two unrelated people).  This makes sense, as most statistical methods assume independent samples and if you have, say, two sisters in the analysis, they're not really independent.  However, the tools for assessing relatedness are...ahem...related to methods for assessing population structure, which is a complex topic that I'll discuss later.
